# -*- coding: utf-8 -*-
"""Submission Projek 1 - Rizki Tetania

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10GWqEtGZObA2wZPUlIhcICzoEO9o5J4e

# Deteksi Penyakit Liver Menggunakan Pendekatan Machine Learning

[Dataset](https://www.kaggle.com/datasets/uciml/indian-liver-patient-records?resource=download) ini memiliki **583** data pasien dengan berbagai variabel dan kategori kelas setiap pasien. Variabel yang dimaksud di sini adalah fitur non-numerik seperti Gender, serta fitur numerik seperti Age, Total Bilirubin, Direct Bilirubin, Alkaline Phosphotase, Alamine Aminotransferase, Aspartate Aminotransferase, Total Proteins, Albumin dan Albumin & Globulin Ratio. Kesepuluh fitur ini adalah fitur yang akan digunakan dalam menemukan pola pada data, sedangkan Class merupakan fitur target.

**Rumusan Masalah**

Berdasarkan latar belakang yang diuraikan sebelumnya, maka projek ini dikembangkan untuk menjawab permasalahan berikut.
- Dari serangkaian faktor atau variabel yang ada, maka adakah faktor yang berhubungan satu sama lain yang penyebab penyakit Liver?
- Model apa yang paling tepat untuk mendeteksi penyakit Liver sejak dini? 

**Tujuan**

Jawaban dari permasalahan diatas, dapat dijelaskan sebagai berikut.
- Mengetahui faktor yang saling berkorelasi dalam menyebabkan seseorang menderita penyakit Liver.
- Membuat model Machine Learning yang dapat mendeteksi penyakit Liver sedini mungkin berdasarkan faktor atau variabel-variabel yang ada.

**Solution Statement**

Berikut merupakan solusi untuk Goals atau tujuan yang ingin dicapai pada projek.
- Melakukan Exploratory Data Analysis (EDA) menggunakan library *corr* untuk mengetahui korelasi antar varriabel.
- Melakukan perbandingan model yang terbentuk berdasarkan empat metode regresi yaitu *K-Nearest Neighbour (KNN)*, *Support Vector Machine (SVM)*, *Logistic Regression*, dan *Random Forest* (RF).

**Metodologi**

Metodologi pada proyek ini adalah: membangun model regresi dengan kelas kategori pasien sebagai target.


**Metrik**

Metrik digunakan untuk mengevaluasi seberapa baik model Anda dalam memprediksi harga. Untuk kasus regresi, beberapa metrik yang biasanya digunakan adalah Mean Squared Error (MSE) atau Root Mean Square Error (RMSE). Secara umum, metrik ini mengukur seberapa jauh hasil prediksi dengan nilai yang sebenarnya. Kita akan bahas lebih detail mengenai metrik ini di modul Evaluasi.

Berdasarkan model yang terbentuk, akan dipilih satu model yang memiliki nilai kesalahan prediksi terkecil. Dengan kata lain, kita akan membuat model seakurat mungkin, yaitu model dengan nilai kesalahan sekecil mungkin.

**Tahap Preprocessing**

tahap ini terdiri dari:

- Data loading
- Exploratory Data Analysis - Deskripsi Variabel
- Exploratory Data Analysis - Menangani Missing Value dan Outliers
- Exploratory Data Analysis - Univariate Analysis
- Exploratory Data Analysis - Multivariate Analysis


---


# Tahap Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.
# import library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

#load data
data = pd.read_csv('indian_liver_patient.csv')
data

"""**Exploratory Data Analysis(EDA)**

Tahapan ini bertujuan untuk memahami data, melalui diskripsi data maupun secara visualisasi.

*Perlu diingat bahwa penerapan teknik EDA tentu berbeda antara satu data dan lainnya.*

"""

#EDA deskripsi variabel 
data.info()

"""Variabel-variabel pada *Indian Patient Liver* dataset dapat dijelaskan sebagai berikut.
- *Age* : Usia pasien
- *Gender* :Jenis kelamin pasien
- *Total Bilirubin* : Jumlah sel darah merah dihati (mg/dl) 
- *Direct Bilirubin* : Bilirubin bebas (mg/dl)
- *Alkaline Phosphotase* : Enzim yang terkandung dari usus
- *Alamine Aminotransferase* : Enzim yang terkandung didalam hati
- *Aspartate Aminotransferase* : Enzim protein yang berada didalam hati
- *Total Protiens* : Serum protein yang terdapat didalam hati (g/dl)
- *Albumin* : Sintesa protein didalam hati
- *Albumin and Globulin Ratio* : Menunjukkan perbandingan rasio albumin dan globulin didalam hati
- *Dataset / Class* : Kategori kelas pasien yang menderita dan tidak menderita penyakit Liver.

Berdasarkan sebelas variabel yang terdapat pada dataset tersebut, terdapat empat variabel penting yang menentukan pasien menderita penyakit Liber yaitu, peningkatan *Total bilirubin*, peningkatan *Alamine Aminotranferase*, peningkatan *Aspartate Aminotransferase*, dan penurunan kadar *Albumin*.
"""

data.describe()

"""Berdasarkan hasil dari diskripsi data tersebut, menunjukkan bahwa ;
1. Rata-rata usia pasien terindikasi penyakit Liver yaitu 44 tahun, dimana pasien yang paling tertua berusia 90 tahun.
2. 441 pasien berjenis kelamin laki-laki dan 142 pasien berjenis kelamin perempuan.
3. Rata-rata total kadar bilirubin dalam tubuh pasien yaitu sebesar 3,29 g/mol, dimana yang paling tertinggi yaitu 75 g/mol. Karena antara nilai kandungan bilirubin tertinggi dengan nilai rata-rata terpaut cukup jauh, maka hal tersebut mengindikasikan terdapat data outlier pada variabel Total  bilirubin.
4. Sedangkan rata-rata kadar albumin pasien yaitu sebesar 3,14 mg/dl dan yang tertinggi sebesar 5,50 mg/dl, dimana kisaran normal albumin adalah 0-8 mg/dl.

"""

data_null = data['Albumin_and_Globulin_Ratio'].isnull().sum() #cek missing value atau data.isna().sum()
print('jumlah data yang kosong di fitur Albumin_and_Globulin_Ratio: ', data_null)

"""Ternyata terdapat data yang missing value yaitu pada data Albumin and Globulin Ratio. Memiliki 4 data yang hilang. Sehingga diperlukan suatu metode untuk mengatasi hal tersebut.

Terdapat 3 cara dalam mengatasi **missing value**;
1. Dibiarkan
2. Mengahapus varaibel atau baris data yang mengandung missing value==> .dropna()
3. Mensubtitusikan nilai mean/ median/ modus kedalam variabel atau baris data yang mengandung missing value==> .fillna()



 *Albumin merupakan protein utama yang memiliki struktur sederhana dengan jumlah sedikit di dalam sel, sedangkan globulin merupakan protein sederhana dengan jumlah banyak di dalam plasma dan sel*. *missing value* pada variabel tersebut akan diisi dengan nilai rata-rata dari kadar albumin dan globulin seluruh pasien.

 **Selanjutnya, mari kita atasi variabel yang terdapat *missing value* tersebut dengan cara mensubtitusikan nilai *mean*.**

"""

#mengatasi missing value
rata_ag = data['Albumin_and_Globulin_Ratio'].mean()
data['Albumin_and_Globulin_Ratio'] = data['Albumin_and_Globulin_Ratio'].fillna(rata_ag)
cek = data['Albumin_and_Globulin_Ratio'].isna().sum()
print('jumlah missing value pada fitur Albumin_and_Globulin_Ratio saat ini : ', cek )

data.describe()

"""Okey, karena nilai *missing value* variabel *Albumin and Globulin Ratio* sudah teratasi, maka mari kita lanjutkan ke tahapan selanjutnya yaitu menangani *outliers*.

**EDA Mengatasi Outliers**

Outlier merupakan data yang memiliki karakteristik yang berbeda jauh dari observasi-observasi lainnya dan muncul dalam bentuk nilai ekstrim baik untuk variabel tunggal atau variabel kombinasi. Ia adalah **hasil pengamatan yang kemunculannya sangat jarang dan berbeda dari data hasil pengamatan lainnya**. 

Ada beberapa teknik untuk menangani outliers, antara lain:

- Hypothesis Testing
- Z-score method
- IQR Method

Pada kasus ini, Anda akan mendeteksi outliers dengan teknik visualisasi data (boxplot). Kemudian, akan menangani outliers dengan teknik IQR method.

Sekarang, mari kita visualisasikan data dengan boxplot untuk mendeteksi outliers pada beberapa fitur numerik.
"""

#categorical columns
cat = ['Class', 'Gender']
#continuous columns
conti =[ 'Age', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin','Albumin_and_Globulin_Ratio']

#cek outlier menggunakan grafik boxplot
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,12))
for i in range(0, len(conti)):
  plt.subplot(6, 5 , i+1)
  sns.boxplot(data[conti[i]], color='purple', orient='v')
  plt.tight_layout()
# sns.boxplot(data['Asportate_Aminotransferase'], color='green', orient='v')

"""Dapat diketahui bahwa 7 dari 9 variabel kontinu memiliki data outlier. Sehingga untuk mengatasi hal tersebut, akan diatasi menggunakan metode IQR. Data yang berada diluar nilai Q3 dan Q1 akan dianggap sebagai data outlier dan akan dihapus."""

#mengatasi outlier
print(f'Jumlah baris sebelum memfilter outlier: {len(data)}')

## CODE HERE
filtered_entries = np.array([False] * len(data))
for col in ['Total_Bilirubin', 'Direct_Bilirubin', 'Alamine_Aminotransferase', 'Aspartate_Aminotransferase', 'Total_Protiens']:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    low_limit = Q1 - (IQR * 1.5)
    high_limit = Q3 + (IQR * 1.5)

    filtered_entries = ((data[col] < low_limit) | (data[col] > high_limit)) | filtered_entries
    
data = data[~filtered_entries]
print(f'Jumlah baris setelah memfilter outlier: {len(data)}')

#cek apakah ada data duplikat
print(data.duplicated().sum())
#apabila terdapat data duplicate, maka dapat diatasi dengan cara menghilangkan data tersebut
data.drop_duplicates(inplace=True)
#cek
print(data.duplicated().sum())

data.shape

"""Setelah data outliers sudah diatasi, ukuran data menjadi  440 baris dari 11 kolom. Langkah selanjutnya yaitu melakukan EDA Univariate Analysis.

**EDA Univariate Analysis**
"""

#berikut adalah sebaran nilai pada variabel kategori
for col in cat:
  print(f'''Value count kolom {col}: ''')
  print(data[col].value_counts())
  print()

import matplotlib.pyplot as plt
labels = 'male','female'
data['Gender'].value_counts().plot.pie(explode=[0.1,0.1],labels=labels, autopct='%1.1f%%',shadow=True,figsize=(5,5));
plt.title("PROPORSI JENIS KELAMIN PASIEN(%)")

"""Grafik tersebut memberikan informasi bahwa 72,3% pasien berjenis kelamin laki-laki, sedangkan 27,7% lainnya merupakan perempuan."""

labels1 = 'Liver','Non-Liver'
data['Class'].value_counts().plot.pie(explode=[0.1,0.1],labels=labels1, autopct='%1.1f%%',shadow=True,figsize=(5,5));
plt.title("PROPORSI KATEGORI PASIEN(%)")

"""Sedangkan untuk Grafik Proporsi Kategori Pasien menjelaskan 63,3% merupakan pasien yang terindikasi menderita penyakit Liver, sedangkan 36,7% pasien tidak terindikasi penyakit Liver."""

#numerical features
data.hist(bins=50, figsize=(20,15))
plt.show()

"""Informasi yang ditunjukkan pada Histogram diatas yaitu :

- Variabel *Age, Total Proteins, Albumin* dan *Albumin and Globulin Ratio* memiliki distribusi data hampir mendekai distribusi normal
- Sedangkan pada variabel *Total Bilirubin, Direct Bilirubin, Alkaline Phosphotase, Alamine Aminotransferase, Aspartate Aminotransferase* memiliki distribusi yang miring (*skewness*) ke kanan. Hal ini akan berimplikasi pada model.

Untuk EDA selanjutnya yaitu menjelaskan tentang EDA Multivariate Analysis.

**EDA Multivariate Analysis**
"""

#numerical features

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(data, diag_kind = 'kde')

"""*Fungsi pairplot dari library seaborn menunjukkan relasi pasangan dalam dataset.* Pada kasus ini, kita akan melihat relasi antara semua fitur numerik. 

Pada pola sebaran data grafik pairplot tersebut, terlihat bahwa terdapat beberapa variabel yang memiliki korelasi yang cukup tinggi satu sama lain, variabel tersebut erdiri dari :
- *Total Bilirubin* dan *Direct Bilirubin*
- *Alamine Aminotransferase* dan *Aspartate Aminotransferase* 
- *Albumin* dan *Albumin and Globulin Ratio*

Hubungan atau korelasi yang cukup tinggi antar variabel independen dapat mengindikasikan bahwa terdapat **Multikolinearitas**. Untuk mengevaluasi skor korelasinya, gunakan fungsi corr().

"""

plt.figure(figsize=(10,8))
correlation_matrix = data.corr().round(2)
 
# untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title('Correlation Matrix untuk Fitur Numerik', size=20)

"""Nah, pada grafik korelasi di atas. **Jika kita amati, variabel 'Total Bilirubin' dan ‘'Direct Bilirubin' memiliki skor korelasi yang besar (di atas 0.9) dengan antar satu sama lain**. Artinya, fitur tersebut terindikasi memiliki Multikolinearitas ([Multikolinearitas](https://www.statistikian.com/2016/11/multikolinearitas.html) adalah sebuah situasi yang menunjukkan adanya korelasi atau hubungan kuat antara dua variabel bebas atau lebih dalam sebuah model regresi). *Sehingga untuk tahap preparation akan dilakukan reduksi dimensi data untuk kedua variabel tersebut.*

# Data Preparation

Data *preparation* atau biasa disebut sebagai tahapan data *preprocessing* merupakan tahapan yanng dilakukan karena dapat memberikan fungsi atau manfaat pada data mining. Proses ini utamanya dilakukan untuk memastikan kualitas data baik sebelum digunakan saat analisis data. Terdapat empat tahap yang digunakan dalam proses *preprocessing* projek, yaitu ;

- Label Encoding fitur kategori.
- Reduksi dimensi dengan Principal Component Analysis (PCA).
- Data traininig dan Data Testing.
- Standarisasi.

---



**Label Encoding Fitur Kategori**

Tahapan ini bertujuan untuk merubah data kategorik menjadi data numerik, dimana library Python yang paling umum digunakan adalah Scikit-Learn. Terdapat dua variabel yang bertipe data kategori yaitu variabel Gender dan Class. Kedua variabel tersebut akan dirubah menjadi numerik menggunakan fungsi LabelEncoder yang terdapat pada *library* sklearn.preprocessing. Setelah fungsi LabelEncoder dibentuk, maka kemudian lakukan proses fit_transform() dengan kolom variabel yang akan dirubah.
"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data = data.apply(le.fit_transform)
data.head()

"""Sekarang variabel kategori yang kita miliki, sudah menjadi varibel kategorik.

**Reduksi Dimensi dengan PCA**

[*Principal Component Analysis* (PCA)](https://dqlab.id/analisis-pca-sederhanakan-data-dengan-reduksi-dimensi-menggunakan-r) adalah salah satu metode reduksi dimensi pada machine learning. PCA akan memilih variabel-variabel yang mampu menjelaskan sebagian besar variabilitas data. PCA mengurangi dimensi dengan membentuk variabel-variabel baru yang disebut Principal Components. . Principal Components yang merupakan kombinasi linier dari variabel-variabel lama. Penghitungan Varians dan Principal Component ini dapat dilakukan dengan menggunakan konsep nilai eigen (eigenvalue) dan vektor eigen (eigenvector) dari ilmu Aljabar Linier.

---
Manfaat dari penggunaan PCA yaitu, mengatasi multikolinearitas; mereduksi jumlah variabel yang akan dimasukkan kedalam model; jumlah variabel yang lebih sedikit tenu akan menyederhanakan model; dan juga mempercepat komputasi.
Penggunaan PCA pada projek ini, digunakan untuk mereduksi variabel *Total Bilirubin* dan *Direct Bilirubin* dibentuk menjadi satu dimensi, karena kedua variabel memiliki nilai korelasi yang cukup tinggi. Sehingga mereduksinya menjadi satu variabel atau dimensi, dapat mambantu mempercepat proses 
komputasi dan yang lainnya.
"""

sns.pairplot(data[['Direct_Bilirubin', 'Total_Bilirubin']], plot_kws={'s':3});

"""Terbukti pada grafik tersebut, bahwa variabel atau fitur *Total Bilirubin* dan *Direct Bilirubin* memiliki korelasi yang cukup tinggi. Sehingga untuk selanjutnya, melakukan PCA pada kedua fitur tersebut dengan mengaplikasikan PCA dari library scikit learn .

- Parameter n_components merupakan jumlah komponen atau dimensi, dalam kasus ini akan kita bentuk menjadi satu dimensi saja, sehingga n_components=1.
- Sedangkan, parameter random_state berfungsi untuk mengontrol random number generator yang digunakan. Berapa pun nilai integer yang kita tentukan --selama itu bilangan integer, ia akan memberikan hasil yang sama setiap kali dilakukan pemanggilan fungsi (dalam kasus kita, class PCA). 
- Tambahkan fitur baru ke dataset dengan nama 'dimension' dan lakukan proses transformasi.
- Drop kolom *Total Bilirubin* dan *Direct Bilirubin*

---
*Menentukan parameter random_state bertujuan untuk dapat memastikan bahwa hasil pembagian dataset konsisten dan memberikan data yang sama setiap kali model dijalankan. Jika tidak ditentukan, maka tiap kali melakukan split, kita akan mendapatkan data train dan tes berbeda. Hal ini berpengaruh terhadap akurasi model ML yang menjadi berbeda tiap kali di-run*. 

---

"""

from sklearn.decomposition import PCA

pca = PCA(n_components=1, random_state=123)
pca.fit(data[['Direct_Bilirubin', 'Total_Bilirubin']])
data['dimension'] = pca.transform(data.loc[:, ('Direct_Bilirubin', 'Total_Bilirubin')]).flatten()
data.drop(['Direct_Bilirubin', 'Total_Bilirubin'], axis=1, inplace=True)

# proporsi informasi dari komponen
pca.explained_variance_ratio_.round(3)

data.head()

data.shape

"""**Data Training dan Data Testing**

[Training atau Testing Split](https://ilmudatapy.com/evaluasi-model-machine-learning-dengan-train-test-split/) merupakan salah satu metode yang digunakan untuk mengevaluasi peforma model Machine Learning. Metode evaluasi model ini membagi dataset menjadi dua bagian yatu bagian yang digunakan untuk *training* data dan bagian untuk *testing* data dengan proporsi tertentu.

---
**Train data** digunakan untuk *fit* model Machine Learning, sedangkan *test data* digunakan untuk mengevaluasi hasil *fit* model tersebut. Metode train/test split ini akan memberikan hasil prediksi yang lebih akurat untuk data baru atau data yang belum pernah di- *train*. Karena data *testing* tidak digunakan untuk melatih model, maka model tidak mengetahui *outcome* dari data tersebut. Pada projek ini, proporsi pembagian data *training* : *testing* sebesar 90:10.
"""

#train test split
from sklearn.model_selection import train_test_split
X = data.drop(['Class'], axis=1)
y = data['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 123)

# untuk menge-cek jumlah masing-masing split
print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""**Standarisasi**

[Standarisasi](https://anzihory.medium.com/normalisasi-vs-standarisasi-101093633e18) adalah teknik lain dalam melakukan perubahan skala, dimana data yang dimiliki akan diubah sehingga memiliki nilai rata-rata sama dengan nol (terpusat) dan standar deviasi sama dengan satu. Proses standarisasi pada Python, dapat dilakukan dengan cara meng- *import* StandardScaler dari sklearn.preprocessing.
"""

# standarisasi pada data training

from sklearn.preprocessing import StandardScaler
 
numerical_features = [ 'Age', 'Gender', 'Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin','Albumin_and_Globulin_Ratio','dimension']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""Seperti yang telah disebutkan sebelumnya, proses standarisasi mengubah nilai rata-rata (mean) menjadi 0 dan nilai standar deviasi menjadi 1. Untuk mengecek nilai mean dan standar deviasi pada setelah proses standarisasi, jalankan kode ini:"""

X_train[numerical_features].describe().round(4)

""" Perhatikan tabel di atas, sekarang nilai mean = 0 dan standar deviasi = 1. Sampai di tahap ini, data kita telah siap untuk dilatih menggunakan model machine learning. 
 
 ---

# Model Development dengan K-Nearest Neighbor 

[K-Nearest Neighbors atau KNN](https://medium.com/@annisaayunda/knn-k-nearest-neighbors-with-forestfires-dataset-c810e603869d) adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran (train data sets), yang diambil dari k tetangga terdekatnya ( *nearest neighbors* ). Dengan k merupakan banyaknya tetangga terdekat.
KNN digunakan untuk klasifikasi dan regresi. KNN hanya sebuah perkirakan, dan semua perhitungan ditunda sampai klasifikasi. Sebuah bobot dapat menetapkan apakah tetangga dekat lebih berpengaruh daripada tetangga yang lebih jauh. Kelebihan metode KNN yaitu mudah diterapkan dan tidak perlu membuat asumsi data sebelumnya. Sedangkan kekurangan metode ini yaitu membutuhkan cukup banyak waktu untuk melakukan prediksi, karena menghitung selisih setiap titik data. Tahapan Langkah algoritma metode [KNN](https://medium.com/@aida.mahmudah171/melakukan-prediksi-kelulusan-dengan-knn-di-jupyter-notebook-4c7c707acd2c) :
1. Menentukan parameter k (jumlah tetangga paling dekat).
2. Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan.
3. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah)
4. .Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k)
5. Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek.
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'SVR', 'LogisticRegression', 'RandomForest'])

# melatih data dengan KNN
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)

models.loc['train_mse', 'knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""# Model Development dengan Supoort Vector Machine (SVM)

[Support Vector Machine](https://iansuryap.medium.com/classification-with-support-vector-machine-svm-methode-7ad33d8951b3) adalah suatu teknik untuk melakukan prediksi, baik dalam hal kasus klasifikasi maupun regresi. Metode SVM berada dalam satu kelas dengan *Artificial Neural Network* (ANN) dalam hal fungsi dan kondisi permalasahan yang bisa diselesaikan. Teknik SVM yang digunakan untuk menjawab permasalahan yaitu menggunakan *Support Vector Regression) (SVR).

[Support Vector Regression](https://medium.com/@nurfauziah_uci/support-vector-regression-pada-harga-saham-netflix-dengan-python-2cc9deb169da) adalah salah satu metode regresi dengan menggunakan Machine Learning yang sangat populer. Konsep dari SVR adalh membuat sebuah *hyperplane* yang mendekati titik-titik data yang akan diperediksi, sehingga diperoleh estimasi yang memiliki nilai error yang sangat kecil.
"""

from sklearn.svm import SVR #MODEL SVM
svr = SVR(kernel='linear') 
svr.fit(X_train,y_train)
models.loc['train_mse', 'svr'] = mean_squared_error(y_pred = svr.predict(X_train), y_true=y_train)

"""# Model Development dengan Logistic Regression

[Logistic Regression](https://medium.com/@rismitawahyu/comparing-analysislogistic-regression-k-nearest-neighbors-k-nn-and-support-vector-machine-svm-67a5d0cc4091) adalah salah satu metode statistika yang menggambarkan hubungan antara variabel respon (y) dengan satu atau lebih variabel prediktor (x), dimana variabel respon dalam regresi logistik adalah biner atau dikotomi yaitu hanya memiliki dua kategori. Hasil untuk setiap pengamatan dapat diklasifikasikan sebagai “sukses” atau “gagal”. Klasifikasi ini diwakili dengan y = 1 untuk hasil pengamatan “sukses” dan y = 0 untuk hasil pengamatan “gagal”. Regresi logistik adalah cara statistik yang kuat dari pemodelan hasil binomial dengan satu atau lebih variabel penjelas. 
Metode ini bekerja dengan cara mengukur hubungan antara variabel target (yang ingin diprediksi) dan variabel input (fitur yang digunakan) dengan fungsi logistik. Probabilitas akan dihitung menggunakan fungsi sigmoid untuk mengubah nilai-nilai tadi menjadi 0 atau 1.
"""

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression()
LR.fit(X_train, y_train)
models.loc['train_mse', 'LogisticRegression'] = mean_squared_error(y_pred = LR.predict(X_train), y_true=y_train)

"""# Model Development dengan Random Forest

Algoritma [Random Forest](https://algorit.ma/blog/cara-kerja-algoritma-random-forest-2022/) disebut sebagai salah satu algoritma machine learning terbaik, sama seperti Naïve Bayes dan Neural Network. Random Forest adalah kumpulan dari decision tree atau pohon keputusan. Algoritma ini merupakan kombinasi masing-masing tree dari decision tree yang kemudian digabungkan menjadi satu model. Biasanya, Random Forest dipakai untuk masalah regresi dan klasifikasi dengan kumpulan data yang berukuran besar. 

---
Random Forest bekerja dengan membangun beberapa decision tree dan menggabungkannya demi mendapatkan prediksi yang lebih stabil dan akurat. ‘Hutan’ yang dibangun oleh Random Forest adalah kumpulan decision tree di mana biasanya dilatih dengan metode bagging. Ide umum dari metode bagging adalah kombinasi model pembelajaran untuk meningkatkan hasil keseluruhan
Algoritma Random Forest meningkatkan keacakan pada model sambil menumbuhkan tree. Alih-alih mencari fitur yang paling penting saat memisahkan sebuah node, Random Forest mencari fitur terbaik di antara subset fitur yang acak. Alhasil, cara ini menghasilkan keragaman yang luas dan umumnya menghasilkan model yang lebih baik.
"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor
 
# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['train_mse', 'RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""- n_estimator: jumlah trees (pohon) di forest. Di sini kita set n_estimator=50.
- max_depth: kedalaman atau panjang pohon. Ia merupakan ukuran seberapa banyak pohon dapat membelah (splitting) untuk membagi setiap node ke dalam jumlah pengamatan yang diinginkan.
- random_state: digunakan untuk mengontrol random number generator yang digunakan. 
- n_jobs: jumlah job (pekerjaan) yang digunakan secara paralel. Ia merupakan komponen untuk mengontrol thread atau proses yang berjalan secara paralel. n_jobs=-1 artinya semua proses berjalan secara paralel.

# Evaluasi Model
"""

# proses scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN', 'SVR', 'LogisticRegression', 'RandomForest'])
 
# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'SVR': svr, 'LogisticRegression': LR, 'RandomForest': RF}
 
# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
  mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
  mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3
 
# Panggil mse
mse

# plot metric dengan bar chart
fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dari gambar di atas, terlihat bahwa, model Random Forest (RF) memberikan nilai eror yang paling kecil. Sedangkan model dengan algoritma Logistic Regression memiliki eror yang paling besar. Model inilah yang akan kita pilih sebagai model terbaik untuk melakukan deteksi penyakit Liver."""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

"""Terlihat bahwa prediksi dengan Random Forest (RF) memberikan hasil yang paling mendekati."""